{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analytics Groupwork1\n",
    "\n",
    "## Group G: Joanna Andari, Karim Awad, Jiye Ren, Nirbhay Sharma, Qiuyue Zhang, Xiaoyan Zhou\n",
    "\n",
    "2017/12/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (5 points) (You will get the data for this on 25 Nov 2017 8PM): The data collected will invariably be fraught with problems. We proceed nevertheless. Use built-in functions in NetworkX on the data HW2_ who_talks_to_whom.txt to do an organizational network analysis report (1 page max)---essentially calculating centrality measures (try at least one eigenvalue based one) and clustering coefficients and gaining some insight into the network. The objective for me (your client) is to identify who are the leaders and opinion-makers in your cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_sent.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d43263bcc88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Part 1: Data Cleaning & Drawing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0msent_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_sent.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mreceived_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_received.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mavg_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_avg.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\64254\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\64254\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\64254\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\64254\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\64254\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_sent.csv' does not exist"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import collections\n",
    "import seaborn as sns \n",
    "\n",
    "# Part 1: Data Cleaning & Drawing\n",
    "\n",
    "sent_mat = pd.read_csv('/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_sent.csv')\n",
    "received_mat = pd.read_csv('/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_received.csv')\n",
    "avg_mat = pd.read_csv('/Users/Veronique/Desktop/ICBA/Nov_Dec Term/Network Analytics/Homework/Hw2/HW2_who_talks_to_whom_avg.csv')\n",
    "\n",
    "# re-index the matrix\n",
    "# col_naming_dict = {str(i+1):i for i in list(range(81))}\n",
    "# sent_mat.rename(columns = col_naming_dict)\n",
    "# received_mat\n",
    "\n",
    "# create node list\n",
    "node_no_dict = {i:str(i+1) for i in list(range(81))}\n",
    "\n",
    "sent_node_list = list()\n",
    "for i in list(range(81)):\n",
    "    for j in list(range(81)):\n",
    "        if sent_mat.iloc[i, j] != 0:\n",
    "            sent_node_list.append([node_no_dict[i], node_no_dict[j], sent_mat.iloc[i, j]])\n",
    "\n",
    "received_node_list = list()\n",
    "for i in list(range(81)):\n",
    "    for j in list(range(81)):\n",
    "        if received_mat.iloc[i, j] != 0:\n",
    "            received_node_list.append([node_no_dict[i], node_no_dict[j], received_mat.iloc[i, j]])\n",
    "\n",
    "avg_node_list = list()\n",
    "for i in list(range(81)):\n",
    "    for j in list(range(81)):\n",
    "        if avg_mat.iloc[i, j] != 0:\n",
    "            avg_node_list.append([node_no_dict[i], node_no_dict[j], avg_mat.iloc[i, j]])\n",
    "\n",
    "# create DiGraph\n",
    "G_sent = nx.DiGraph()\n",
    "G_sent.add_weighted_edges_from(sent_node_list)\n",
    "\n",
    "G_received = nx.DiGraph()\n",
    "G_received.add_weighted_edges_from(received_node_list)\n",
    "\n",
    "G_total = nx.DiGraph()\n",
    "G_total.add_weighted_edges_from(avg_node_list)\n",
    "\n",
    "# draw the graphs\n",
    "# sent (weight > k)\n",
    "k_sent = 0\n",
    "\n",
    "node_color_vec_sent = dict()\n",
    "for key in G_sent.nodes():\n",
    "    if len(G_sent.neighbors(key)) >= np.percentile([len(G_sent.neighbors(i)) for i in G_sent.nodes()], 90):\n",
    "        node_color_vec_sent[key] = 'r'\n",
    "    elif len(G_sent.neighbors(key)) >= np.percentile([len(G_sent.neighbors(i)) for i in G_sent.nodes()], 75):\n",
    "        node_color_vec_sent[key] = 'hotpink'\n",
    "    else:\n",
    "        node_color_vec_sent[key] = 'pink'\n",
    "        \n",
    "data_sent = {'edgelist': [sent_node_list[i] for i in list(range(2046))],\n",
    "            'tail': [sent_node_list[i][0] for i in list(range(2046))],\n",
    "            'head': [sent_node_list[i][1] for i in list(range(2046))],\n",
    "            'weight': [sent_node_list[i][2] for i in list(range(2046))],\n",
    "            'num_neighbor': [len(G_sent.neighbors(i[0])) for i in sent_node_list],\n",
    "            'color': [node_color_vec_sent[i[0]] for i in sent_node_list]}\n",
    "node_attr_sent = DataFrame(data_sent)\n",
    "\n",
    "node_attr_sent_draw = DataFrame(columns = ['edgelist', 'tail', 'head', 'weight', 'num_neighbor', 'color'])\n",
    "for i in list(range(2046)):\n",
    "    if node_attr_sent['weight'][i] > k_sent:\n",
    "        node_attr_sent_draw = node_attr_sent_draw.append(node_attr_sent.loc[i], ignore_index=True)\n",
    "\n",
    "nx.draw(G_sent, \n",
    "        pos = nx.spring_layout(G_sent, k = 0.5, iterations = 80, scale = 900),\n",
    "        edgelist = list(node_attr_sent_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [(len(G_sent.neighbors(i)) - 2) * 35 for i in G_sent.nodes()],\n",
    "        node_color = list(node_color_vec_sent.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        width = [float(d['weight'] / 24 ) for (u, v, d) in G_sent.edges(data = True)],\n",
    "        alpha = 0.4,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Main Information Sender Network',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Main Information Sender Network',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "# received (weight > k)\n",
    "k_received = 0\n",
    "\n",
    "node_color_vec_received = dict()\n",
    "for key in G_received.nodes():\n",
    "    if len(G_received.neighbors(key)) >= np.percentile([len(G_received.neighbors(i)) for i in G_received.nodes()], 90):\n",
    "        node_color_vec_received[key] = 'r'\n",
    "    elif len(G_received.neighbors(key)) >= np.percentile([len(G_received.neighbors(i)) for i in G_received.nodes()], 75):\n",
    "        node_color_vec_received[key] = 'hotpink'\n",
    "    else:\n",
    "        node_color_vec_received[key] = 'pink'\n",
    "        \n",
    "data_received = {'edgelist': [received_node_list[i] for i in list(range(1985))],\n",
    "            'tail': [received_node_list[i][0] for i in list(range(1985))],\n",
    "            'head': [received_node_list[i][1] for i in list(range(1985))],\n",
    "            'weight': [received_node_list[i][2] for i in list(range(1985))],\n",
    "            'num_neighbor': [len(G_received.neighbors(i[0])) for i in received_node_list],\n",
    "            'color': [node_color_vec_received[i[0]] for i in received_node_list]}\n",
    "node_attr_received = DataFrame(data_received)\n",
    "\n",
    "node_attr_received_draw = DataFrame(columns = ['edgelist', 'tail', 'head', 'weight', 'num_neighbor', 'color'])\n",
    "for i in list(range(1985)):\n",
    "    if node_attr_received['weight'][i] > k_received:\n",
    "        node_attr_received_draw = node_attr_received_draw.append(node_attr_received.loc[i], ignore_index=True)\n",
    "\n",
    "nx.draw(G_received, \n",
    "        pos = nx.spring_layout(G_received, k = 0.6, iterations = 80, scale = 900),\n",
    "        edgelist = list(node_attr_received_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [(len(G_received.neighbors(i)) - 2) * 35 for i in G_received.nodes()],\n",
    "        node_color = list(node_color_vec_received.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        width = [float(d['weight'] / 24 ) for (u, v, d) in G_received.edges(data = True)],\n",
    "        alpha = 0.4,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Main Information Receiver Network',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Main Information Receiver Network',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "# total (weight > k)\n",
    "k_total = 0\n",
    "\n",
    "node_color_vec_total = dict()\n",
    "for key in G_total.nodes():\n",
    "    if len(G_total.neighbors(key)) >= np.percentile([len(G_total.neighbors(i)) for i in G_total.nodes()], 90):\n",
    "        node_color_vec_total[key] = 'r'\n",
    "    elif len(G_total.neighbors(key)) >= np.percentile([len(G_total.neighbors(i)) for i in G_total.nodes()], 75):\n",
    "        node_color_vec_total[key] = 'hotpink'\n",
    "    else:\n",
    "        node_color_vec_total[key] = 'pink'\n",
    "\n",
    "data_total = {'edgelist': [avg_node_list[i] for i in list(range(2057))],\n",
    "              'tail': [avg_node_list[i][0] for i in list(range(2057))],\n",
    "              'head': [avg_node_list[i][1] for i in list(range(2057))],\n",
    "              'weight': [avg_node_list[i][2] for i in list(range(2057))],\n",
    "              'num_neighbor': [len(G_total.neighbors(i[0])) for i in avg_node_list],\n",
    "              'color': [node_color_vec_total[i[0]] for i in avg_node_list]}\n",
    "node_attr_total = DataFrame(data_total)\n",
    "\n",
    "node_attr_total_draw = DataFrame(columns = ['edgelist', 'tail', 'head', 'weight', 'num_neighbor', 'color'])\n",
    "for i in list(range(2057)):\n",
    "    if node_attr_total['weight'][i] > k_total:\n",
    "        node_attr_total_draw = node_attr_total_draw.append(node_attr_total.loc[i], ignore_index=True)\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 85, scale = 900),\n",
    "        # pos = nx.random_layout(G_total),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [(len(G_total.neighbors(i)) - 2) * 35 for i in G_total.nodes()],\n",
    "        node_color = list(node_color_vec_total.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        width = [float(d['weight'] / 18 ) for (u, v, d) in G_sent.edges(data = True)],\n",
    "        alpha = 0.4,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Total Conversation Network',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Total Conversation Network',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "# Part 2: Network Analysis\n",
    "# essentially calculating centrality measures (try at least one eigenvalue based one)\n",
    "# degree centrality measure\n",
    "degree_ctr_total = nx.degree_centrality(G_total)\n",
    "in_degree_ctr_total = nx.in_degree_centrality(G_total)\n",
    "out_degree_ctr_total = nx.out_degree_centrality(G_total)\n",
    "\n",
    "degree_ctr_sent = nx.degree_centrality(G_sent)\n",
    "degree_ctr_received = nx.degree_centrality(G_received)\n",
    "\n",
    "DC_Counter = collections.Counter(degree_ctr_total)\n",
    "plt.hist([value for key, value in DC_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype = \"stepfilled\")\n",
    "plt.title('Histogram of Degree Centrality Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of Degree Centrality Distribution')\n",
    "\n",
    "node_color_vec_DC = dict()\n",
    "for key in G_total.nodes():\n",
    "    if degree_ctr_total[key] >= np.percentile(list(degree_ctr_total.values()), 95):\n",
    "        node_color_vec_DC[key] = 'midnightblue'\n",
    "    elif degree_ctr_total[key] >= np.percentile(list(degree_ctr_total.values()), 75):\n",
    "        node_color_vec_DC[key] = 'dodgerblue'\n",
    "    else:\n",
    "        node_color_vec_DC[key] = 'lightskyblue'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 55, scale = 3000),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [3000 * i ** 2 - 300 for i in list(degree_ctr_total.values())],\n",
    "        node_color = list(node_color_vec_DC.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        alpha = 0.6,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Size and Color by Degree Centrality',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Size and Color by Degree Centrality',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "# betweenness centrality measure\n",
    "betweenness_nodes_ctr_total = nx.betweenness_centrality(G_total) # for nodes\n",
    "betweenness_edges_ctr_total = nx.edge_betweenness_centrality(G_total) # for edges\n",
    "\n",
    "BC_Counter = collections.Counter(betweenness_nodes_ctr_total)\n",
    "plt.hist([value for key, value in BC_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype = \"stepfilled\")\n",
    "plt.title('Histogram of Betweenness Centrality Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of Betweenness Centrality Distribution')\n",
    "\n",
    "node_color_vec_BC = dict()\n",
    "for key in G_total.nodes():\n",
    "    if betweenness_nodes_ctr_total[key] >= np.percentile(list(betweenness_nodes_ctr_total.values()), 95):\n",
    "        node_color_vec_BC[key] = 'midnightblue'\n",
    "    elif betweenness_nodes_ctr_total[key] >= np.percentile(list(betweenness_nodes_ctr_total.values()), 75):\n",
    "        node_color_vec_BC[key] = 'dodgerblue'\n",
    "    else:\n",
    "        node_color_vec_BC[key] = 'lightskyblue'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 55, scale = 3000),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [3000 * i ** 2 - 300 for i in list(degree_ctr_total.values())],\n",
    "        node_color = list(node_color_vec_BC.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        alpha = 0.6,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Size and Color by Betweenness Centrality',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Size and Color by Betweenness Centrality',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "# closeness centrality measure\n",
    "closeness_ctr_total = nx.closeness_centrality(G_total)\n",
    "\n",
    "CC_Counter = collections.Counter(closeness_ctr_total)\n",
    "plt.hist([value for key, value in CC_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype = \"stepfilled\")\n",
    "plt.title('Histogram of Closeness Centrality Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of Closeness Centrality Distribution')\n",
    "\n",
    "node_color_vec_CC = dict()\n",
    "for key in G_total.nodes():\n",
    "    if closeness_ctr_total[key] >= np.percentile(list(closeness_ctr_total.values()), 95):\n",
    "        node_color_vec_CC[key] = 'midnightblue'\n",
    "    elif closeness_ctr_total[key] >= np.percentile(list(closeness_ctr_total.values()), 75):\n",
    "        node_color_vec_CC[key] = 'dodgerblue'\n",
    "    else:\n",
    "        node_color_vec_CC[key] = 'lightskyblue'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 55, scale = 3000),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [15000 * i - 7500 for i in list(closeness_ctr_total.values())],\n",
    "        node_color = list(node_color_vec_CC.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        alpha = 0.6,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Size and Color by Closeness Centrality',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Size and Color by Closeness Centrality',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "# Local metrics comparison\n",
    "plt.hist([(value - min([value for key, value in DC_Counter.items()])) / (max([value for key, value in DC_Counter.items()]) - min([value for key, value in DC_Counter.items()])) for key, value in DC_Counter.items()], \n",
    "           bins = 40, \n",
    "           color = 'olivedrab',\n",
    "           alpha = 0.4,\n",
    "           histtype = \"stepfilled\",\n",
    "           label = 'Degree')\n",
    "plt.hist([(value - min([value for key, value in BC_Counter.items()])) / (max([value for key, value in BC_Counter.items()]) - min([value for key, value in BC_Counter.items()])) for key, value in BC_Counter.items()], \n",
    "           bins = 40, \n",
    "           color = 'seagreen',\n",
    "           alpha = 0.4,\n",
    "           histtype = \"stepfilled\", \n",
    "           label = 'Betweenness')\n",
    "plt.hist([(value - min([value for key, value in CC_Counter.items()])) / (max([value for key, value in CC_Counter.items()]) - min([value for key, value in CC_Counter.items()])) for key, value in CC_Counter.items()], \n",
    "           bins = 40, \n",
    "           color = 'darkcyan', \n",
    "           alpha = 0.4,\n",
    "           histtype = \"stepfilled\",\n",
    "           label = 'Closeness')\n",
    "plt.title('(Normalised) Local Metrics Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.savefig('(Normalised) Local Metrics Distribution')\n",
    "\n",
    "# eigenvector centrality measure\n",
    "eigen_ctr_total = nx.eigenvector_centrality(G_total)\n",
    "\n",
    "EC_Counter = collections.Counter(eigen_ctr_total)\n",
    "plt.hist([value for key, value in EC_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype = \"stepfilled\")\n",
    "plt.title('Histogram of EigenCentrality Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of EigenCentrality Distribution')\n",
    "\n",
    "node_color_vec_EC = dict()\n",
    "for key in G_total.nodes():\n",
    "    if eigen_ctr_total[key] >= np.percentile(list(eigen_ctr_total.values()), 95):\n",
    "        node_color_vec_EC[key] = 'midnightblue'\n",
    "    elif eigen_ctr_total[key] >= np.percentile(list(eigen_ctr_total.values()), 75):\n",
    "        node_color_vec_EC[key] = 'dodgerblue'\n",
    "    else:\n",
    "        node_color_vec_EC[key] = 'lightskyblue'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 55, scale = 3000),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [11000 * i for i in list(eigen_ctr_total.values())],\n",
    "        node_color = list(node_color_vec_EC.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        alpha = 0.6,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Size and Color by EigenCentrality',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Size and Color by EigenCentrality',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "# pangerank\n",
    "pagerank_total = nx.pagerank(G_total)\n",
    "\n",
    "pgrk_Counter = collections.Counter(pagerank_total)\n",
    "plt.hist([value for key, value in pgrk_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype=\"stepfilled\")\n",
    "plt.title('Histogram of PageRank Distribution',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of PageRank Distribution')\n",
    "\n",
    "node_color_vec_pgrk = dict()\n",
    "for key in G_total.nodes():\n",
    "    if pagerank_total[key] >= np.percentile(list(pagerank_total.values()), 95):\n",
    "        node_color_vec_pgrk[key] = 'midnightblue'\n",
    "    elif pagerank_total[key] >= np.percentile(list(pagerank_total.values()), 75):\n",
    "        node_color_vec_pgrk[key] = 'dodgerblue'\n",
    "    else:\n",
    "        node_color_vec_pgrk[key] = 'lightskyblue'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.5, iterations = 55, scale = 3000),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [90000 * i for i in list(pagerank_total.values())],\n",
    "        node_color = list(node_color_vec_pgrk.values()),\n",
    "        edge_color = 'darkgrey',\n",
    "        alpha = 0.6,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Size and Color by PageRank',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Size and Color by PageRank',\n",
    "            bbox_inches='tight')\n",
    "\n",
    "# Global metrics comparison\n",
    "plt.hist([(value - min([value for key, value in EC_Counter.items()])) / (max([value for key, value in EC_Counter.items()]) - min([value for key, value in EC_Counter.items()])) for key, value in EC_Counter.items()], \n",
    "           bins = 40, \n",
    "           color = 'seagreen',\n",
    "           alpha = 0.5,\n",
    "           histtype = \"stepfilled\",\n",
    "           label = 'Eigenvector')\n",
    "plt.hist([(value - min([value for key, value in pgrk_Counter.items()])) / (max([value for key, value in pgrk_Counter.items()]) - min([value for key, value in pgrk_Counter.items()])) for key, value in pgrk_Counter.items()], \n",
    "           bins = 40, \n",
    "           color = 'darkcyan', \n",
    "           alpha = 0.5,\n",
    "           histtype = \"stepfilled\",\n",
    "           label = 'PageRank')\n",
    "plt.title('(Normalised) Global Metrics Distribution', \n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 16)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.savefig('(Normalised) Global Metrics Distribution')\n",
    "\n",
    "\n",
    "# and clustering coefficients and gaining some insight into the network.  \n",
    "clustering_coef = nx.clustering(G_total.to_undirected())\n",
    "avg_clustering_coef = nx.average_clustering(G_total.to_undirected())\n",
    "\n",
    "clstr_Counter = collections.Counter(clustering_coef)\n",
    "plt.hist([value for key, value in clstr_Counter.items()], \n",
    "          bins = 40, \n",
    "          color = 'darkgreen', \n",
    "          histtype=\"stepfilled\")\n",
    "plt.title('Histogram of Node Clustering Coefficient',\n",
    "          fontweight=\"bold\",\n",
    "          fontsize = 16)\n",
    "plt.savefig('Histogram of Node Clustering Coefficient')\n",
    "\n",
    "\n",
    "# find cliques\n",
    "cliques_total = list(nx.find_cliques(G_total.to_undirected()))\n",
    "\n",
    "max_cliques = [i for i in cliques_total if len(i) == max([len(i) for i in cliques_total])]\n",
    "# active ppl in main cliques\n",
    "max_cliques_set = [set(i) for i in max_cliques]\n",
    "ppl_in_all_max_clique = list(reduce(lambda x, y: x.intersection(y), max_cliques_set))\n",
    "\n",
    "max_clique_color_vec = list(node_color_vec_total.values())\n",
    "for i in list(range(len(max_clique_color_vec))):\n",
    "    if str(i + 1) in ppl_in_all_max_clique:\n",
    "        max_clique_color_vec[i] = 'tomato'\n",
    "    else:\n",
    "        max_clique_color_vec[i] = 'peachpuff'\n",
    "\n",
    "nx.draw(G_total, \n",
    "        pos = nx.spring_layout(G_total, k = 0.6, iterations = 85, scale = 900),\n",
    "        edgelist = list(node_attr_total_draw['edgelist']),\n",
    "        with_labels = True,\n",
    "        node_size = [(len(G_total.neighbors(i)) - 2) * 40 for i in G_total.nodes()],\n",
    "        node_color = max_clique_color_vec,\n",
    "        edge_color = 'darkgrey',\n",
    "        width = [float(d['weight'] / 18 ) for (u, v, d) in G_sent.edges(data = True)],\n",
    "        alpha = 0.5,\n",
    "        font_size = 9,\n",
    "        arrows = False)\n",
    "plt.title('Max Clique in the Network',\n",
    "          fontweight = \"bold\",\n",
    "          fontsize = 18)\n",
    "plt.savefig('Max Clique in the Network',\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.[10 or 15 points] The data HW2_tsp.txt contains latitude and longitude data of 38 cities in a country in Africa (Djibouti). Calculate the distance matrix (use an approximation like here which is quite accurate for short distances; or use packages like haversine or geopy). The x and y-cordinates are the latitude and longitude in decimal form multiplied by 1000. EUC_2D means take these as Cc. [Challenge problem., 10 points] The most powerful integer programming solver is Gurobi (along with CPLEX). They give a free one year license if you download and install from a University IP address. Use the most powerful computer you have in your group (cores and memory).artesian co-ordinates. Can also use haversine treating them as longitude and latitude. You are free to use any other functions you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Plot the latitude and longitude as a scatter plot using a drawing package (some options are: matplotlib basemap toolkit (the most advanced, but also the most difficult to use), geopy, gmplot, plotly …). It should look roughly like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [Challenge problem., 10 points] The most powerful integer programming solver is Gurobi (along with CPLEX). They give a free one year license if you download and install from a University IP address. Use the most powerful computer you have in your group (cores and memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Plot the resulting tour on the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (5 points) Show that the greedy algorithm works in finding the optimal (min or max) weighted spanning tree. i.e., you have to show (a) it terminates in finite time---better if you can show polynomial time (b) the tree that it finds is an optimal tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "A tree is an undirected graph that is connected and acyclic. This graph with $n$ nodes has $n-1$ edges and is characterized by having a unique path between any pair of nodes on the tree. A spanning tree of an undirected graph $G$ is a subset of the graph’s edges that form a tree containing (spanning) all nodes. \n",
    "The optimal min/max spanning tree could be found using the naïve algorithm which consists of checking all possible spanning trees in a graph. However, if we have n nodes there are 2^n possible spanning trees thus the algorithm has an exponential running time. \n",
    "\n",
    "Another alternative for finding the min/max spanning tree is through a greedy algorithm (such as Prim’s or Kruskal algorithms). A greedy algorithm consists of picking up the next best solution while maintaining feasibility. In other words, it works in finding the next best local choice with the hope of finding the global optimum. \n",
    "In the case of finding the optimal spanning tree, the greedy algorithm is the best approach as it does not only find the local best solution but gives as well a global optimal solution. This can be shown by the cut property which indicates that in any cut $(S, V-S)$ in a graph $G$, any least weight crossing edge (for a min case) such as edge e incident to nodes $u$ and $v$ with $u \\in S$ and$ V \\notin S$ is in some minimum spanning tree of graph $G$. This can be proven as follows:\n",
    "Consider a minimum spanning tree $T$ of $G$ where $e$ is not in $T$ and has a path between nodes $u$ and $v$. Another minimum spanning tree $T’$ that contains edge e is constructed based on the previous tree $T$.  Since $T$  has a already unique path between u and $v$  we cannot add edge $e$ to $T$ as it creates a cycle (tree should be acyclic). Thus, we assume that there is another edge e’ that connect the cut to T. Thus, for T’ we would have\n",
    " $T'=T∪\\{e\\}-\\{e'\\}$and weight $(T’) =$ Weight $(T) + W(e) – W(e’)$.  Since e is the lowest edge then the weight of T’  is less than the weight of $T$.  Thus, if T is a minimum spanning tree then $T’ =T$ and $W(e) = W(e’)$ proving that T’ is also an MST where the least weight crossing edge is chosen (this can also be shown with a maximum spanning tree taking the highest weight crossing edge). \n",
    "\n",
    "Both Prim’s and Kruskal are based on the cut property to find the min/max spanning tree. The Prim’s algorithm adds edges that have the lowest/highest weight to gradually build up the spanning tree.  In other words, at every step, we need to find a cut and select the lowest/highest weight edge and include it in an empty set containing the spanning tree. This algorithm is very similar to Djikstra as it uses also a priority queue to find the lowest/highest edge and has a running time depending on the type of priority queue used.  A running time of $O (V^2)$ using an array priority queue. This running time is enhanced by using a binary heap and adjacency data structure with a running time of O(E log V) with E for edges and V for vertex. \n",
    "The Kruskal’s algorithm is another approach to solve the min/max spanning tree problem but starts with the globally lowest / highest weight edge where we repeatedly add to the empty set the next lightest/heavier edge that does not produce a cycle. Sorting of the edges in this algorithm has a total running time of $O (E log E)$. After sorting of the edges, we need to iterate through all edges and apply the union and find operations with a running time of $2E·T(Find)+V·T(Union) = O((E+V)log V) = O(E log V)$\n",
    "Thus, the total running time is $O (E log E)$ or $O (E log V)$ which are similar. This running time can be also improved by using a randomized algorithm for minimum cut. \n",
    "\n",
    "References: \n",
    "\tMassachusetts Institute of Technology. (2015). Lecture 12: Greedy Algorithms and Minimum Spanning Tre. Spring Lecture 12, [online]. Available at:$ https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/MIT6_046JS15_writtenlec12.pdf [Accessed 2 Dec 2017].\n",
    "\tS Dasgupta, C.H. Papadimitriou, and U.V. Vazirani. Greedy Algorithms. P. 143 to 151. Available at: file:///C:/Users/Joanna%20Andari/AppData/Local/Packages/Microsoft.MicrosoftEdge_8wekyb3d8bbwe/TempState/Downloads/greedy.pdf [Accessed 2 Dec 2017].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.(5 points)\n",
    "\n",
    "a. Formulate the problem of sending the maximum amount of flow on the following network (the edges have capacities as shown) as a linear program "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\tThe directed weighted graph consists of 7 nodes and 10 edges, to find the maximum flow of the directed graph, a linear programme can be formulated.\n",
    "\n",
    "Knowing that the each weighted edge represent the maximum allowed capacity of flow between each pair of nodes, the following capacity constraints can be formulated:\n",
    "\n",
    "<center>\n",
    "0≤Xsa≤5  \n",
    "\n",
    "0≤Xsb≤13  \n",
    "\n",
    "0≤Xsc≤3  \n",
    "\n",
    "0≤Xad≤3  \n",
    "\n",
    "0≤Xba≤7  \n",
    "\n",
    "0≤Xbc≤5  \n",
    "\n",
    "0≤Xbd≤2  \n",
    "\n",
    "0≤Xbe≤2  \n",
    "\n",
    "0≤Xce≤4  \n",
    "\n",
    "0≤Xed≤9  \n",
    "\n",
    "0≤Xet≤10  \n",
    "\n",
    "0≤Xdt≤5  \n",
    "</center >\n",
    "\n",
    "-Where for symbol “Xij” means the edge connecting nodes “I” and “j”, so “Xsa” means the edge(capacity) between node S and node A.\n",
    "\n",
    "Since the graph is a directed graph, for each node, the outgoing edge carries weights of negative values and incident edge carries weight of positive values. Thus, constraints have to be formulated to regulate the directions of flow:\n",
    "<center>\n",
    "Node S: \t\t\t\t\t\t -Xsc-Xsa-Xsb=-f</center>\n",
    "\n",
    "<center>Node A:\t\t\t\t\t\t Xsa+Xba-Xad=0</center>\n",
    "\n",
    "<center>Node B: \t\t\t\t\t\t Xsb-Xba-Xbc-Xbe-Xbd=0</center>\n",
    "\n",
    "<center>Node C: \t\t\t\t\t\t Xsc+Xbc-Xce=0</center>\n",
    "\n",
    "<center>Node D: \t\t\t\t\t\t Xab+Xbd+Xed-Xdt=0</center>\n",
    "\n",
    "<center>Node E:\t\t\t\t\t\t Xbe+Xce-Xed-Xet=0</center>\n",
    "\n",
    "<center>Node T: \t\t\t\t\t\t Xdt+Xet=f</center>\n",
    "\n",
    "-Symbol “f” refers to the total amount of flow exiting the source(node S) and entering the sink(node T).\n",
    "\n",
    "\n",
    "The final formulated linear programme aiming to find the maximum flow of the network is displayed below:\n",
    "\n",
    "Objective function:\n",
    "\n",
    "Max: |f|\n",
    "\n",
    "Subject to: \n",
    "\n",
    "<center>\n",
    "-Xsc-Xsa-Xsb=-f  \n",
    "\n",
    "Xsa+Xba-Xad=0  \n",
    "\n",
    "Xsb-Xba-Xbc-Xbe-Xbd=0  \n",
    "\n",
    "Xsc+Xbc-Xce=0  \n",
    "\n",
    "Xab+Xbd+Xed-Xdt=0  \n",
    "\n",
    "Xbe+Xce-Xed-Xet=0  \n",
    "\n",
    "Xdt+Xet=f  \n",
    "\n",
    "0≤Xsa≤5  \n",
    "\n",
    "0≤Xsb≤13  \n",
    "\n",
    "0≤Xsc≤3  \n",
    "\n",
    "0≤Xad≤3  \n",
    "\n",
    "0≤Xba≤7  \n",
    "\n",
    "0≤Xbc≤5  \n",
    "\n",
    "0≤Xbd≤2  \n",
    "\n",
    "0≤Xbe≤2  \n",
    "\n",
    "0≤Xce≤4  \n",
    "\n",
    "0≤Xed≤9  \n",
    "\n",
    "0≤Xet≤10  \n",
    "\n",
    "0≤Xdt≤5  \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b. Formulate the dual of the linear program as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual of a maximum flow problem would be a Max-flow min-cut problem, the primal problem is the maximisation of flow, and the dual problem would give the results as the minimum capacity of a cut that separates a network into two disjoint sets, which means the results of the dual problem gives a series of edges with minimum weight compare to other possible cuts.\n",
    "\n",
    "The dual problem of part a) can be expressed as to minimise$\\sum_{u,v \\in E}c_{uv}X_{uv}$ which is the sum of the product of multiplication between the capacity of every edge $C_{uv}$ and binary variable of all edges $X_{uv}$. $X_{uv}$ means the edge connecting node u and v, $X_{uv}$=1 if the cut consists of the edge between node u, v, and $X_{uv}=0 $ otherwise. Cuv means the maximum capacity of edge between node u and v. \n",
    "\n",
    "Knowing the graph is still directed after the cut but nodes would belong to different sets, thus, binary variables are used to find the optimal location of nodes, for example, if node s is in the set S, binary variable $P_s$would equal to 1, if node s is in set T, $P_s=0$.\n",
    "\n",
    "the following constraints for each edge and node will be formulated:  \n",
    "\n",
    "<center>  \n",
    "State binary variable:\t\t\t    Ps-Pt≥1  \n",
    "Edge SA:\t\t\t\t\t\t\tXsa-Ps+Pa≥0  \n",
    "Edge SB:\t\t\t\t\t\t\tXsb-Ps+Pb≥0  \n",
    "Edge SC:\t\t\t\t\t\t\tXsc-Ps+Pc≥0  \n",
    "Edge AD:\t\t\t\t\t\t\tXad-Pa+Pd≥0  \n",
    "Edge BA:\t\t\t\t\t\t\tXba-Pb+Pa≥0  \n",
    "Edge BC:\t\t\t\t\t\t\tXbc-Pb+Pc≥0  \n",
    "Edge AD:\t\t\t\t\t\t\tXbd-Pb+Pd≥0  \n",
    "Edge BE:\t\t\t\t\t\t\tXbe-Pb+Pe≥0  \n",
    "Edge CE:\t\t\t\t\t\t\tXce-Pc+Pe≥0  \n",
    "Edge DE:\t\t\t\t\t\t\tXdt-Pd+Pt≥0  \n",
    "Edge ED:\t\t\t\t\t\t\tXed-Pe+Pd≥0  \n",
    "Edge ET:\t\t\t\t\t\t\tXet-Pe+Pt≥0 <\\center> \n",
    "\n",
    "The final formulated dual linear programme is displayed below.  \n",
    "\n",
    "Minimise:  \n",
    "\n",
    "<center>\n",
    "5Xsa+13Xsb+3Xsc+3Xad+7Xba+5Xbc+2Xbd+2Xbe+4Xce+5Xdt+9Xed+10Xet<\\center>  \n",
    "\n",
    "Subject to:  \n",
    "<center>\n",
    "Xsa-Ps+Pa≥0  \n",
    "Xsb-Ps+Pb≥0  \n",
    "Xsc-Ps+Pc≥0  \n",
    "Xad-Pa+Pd≥0  \n",
    "Xba-Pb+Pa≥0  \n",
    "Xbc-Pb+Pc≥0  \n",
    "Xbd-Pb+Pd≥0  \n",
    "Xbe-Pb+Pe≥0  \n",
    "Xce-Pc+Pe≥0  \n",
    "Xdt-Pd+Pt≥0  \n",
    "Xed-Pe+Pd≥0  \n",
    "Xet-Pe+Pt≥0  \n",
    "Ps-Pt≥1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Solve using a linear programming solver (say Excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "\n",
    "Excel is used as a linear programing solver and the above objective function along with all the constraints have been solved:  \n",
    "\n",
    "Linear optimization of part a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear optimization of part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the maximized flow is $|f|=11$.\n",
    "The minimised cut have a capacity of 11, which equals to the maximum flow, the cut disjoints the graph into two separate sets of nodes: set $S$ and set $T$ (where node $s$ is in set $S$ and node $t$ is in set $T$), and the linear model of dual problem suggests the set $S$ consists node $s, a, b,$ and $c$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)\tAs mentioned in part b) and part c), by solving the dual of maximum-flow linear programme, the minimum capacity of cut can be obtained. \n",
    "\n",
    "Knowing a cut s-t separates graph into disjoint sets $S$ and $T$ with node $s$ in $S$ and node $t$ in $T$, and nodes in $S$ and nodes in $T$ do not include each other. \n",
    "\n",
    "The capacity of the cut,$Cap(S,T)$ means the sum of the capacities from $S$ to $T$, and the minimised capacity of the cut can be obtained from the optimal dual value, and $Cap(S,T)=11$, which equals to the maximum flow in part a) of the question. By setting edges and nodes as binary variables, the results of linear model implies that the minimised capacity of the cut consists of edges $AD, BD, BE,$ and $CE$ (as illustrated in dotted line in the following pitcure, set S consists of nodes $s, a, b$, and $c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

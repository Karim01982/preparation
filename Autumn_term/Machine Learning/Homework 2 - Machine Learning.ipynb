{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Group Assignment 2\n",
    "\n",
    "### Group G: Joanna Andari, Karim Awad, Jiye Ren, Nirbhay Sharma, Qiuyue Zhang, Xiaoyan Zhou\n",
    "#### 09/12/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from astropy.table import Table, Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: \n",
    "Load the data into a Python data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data loading\n",
    "messages = pd.read_csv('Spam.txt',sep='\\t', header=None,\n",
    "                           names=[\"label\", \"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: \n",
    "Pre-process the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lower case. (Please provide the Python code that achieves this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                           messages\n",
      "0   ham  go until jurong point  crazy   available only ...\n",
      "1   ham                      ok lar    joking wif u oni   \n",
      "2  spam  free entry in   a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor    u c already then say   \n",
      "4   ham  nah i don t think he goes to usf  he lives aro...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count                       5572\n",
       "unique                      5146\n",
       "top       sorry  i ll call later\n",
       "freq                          30\n",
       "Name: messages, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data processing\n",
    "def data_processing(p):\n",
    "    remove_number_punc = re.sub(\"[^a-zA-Z]\", \" \", p)\n",
    "    convert_to_lower_letter = remove_number_punc.lower()\n",
    "    return convert_to_lower_letter\n",
    "\n",
    "messages['messages'] = messages['messages'].apply(data_processing)\n",
    "print(messages.head())\n",
    "messages.groupby('label').describe()\n",
    "messages['messages'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: \n",
    "Shuﬄe the messages and split them into a training set (2,500 messages), a validation set (1,000 messages) and a test set (all remaining messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 1000 2073\n"
     ]
    }
   ],
   "source": [
    "#Data shuffling and segmenting\n",
    "random_seed = 100\n",
    "\n",
    "clean_messages_shuffled = shuffle(messages, random_state = random_seed)\n",
    "training_set = clean_messages_shuffled [0:round(len(clean_messages_shuffled.axes[0])/2.2288)]\n",
    "validation_set =clean_messages_shuffled [round(len(clean_messages_shuffled.axes[0])/2.2288):round(len(clean_messages_shuffled.axes[0])/1.592)]\n",
    "test_set=clean_messages_shuffled[round(len(clean_messages_shuffled.axes[0])/1.5925):round(len(clean_messages_shuffled.axes[0])/1)]\n",
    "\n",
    "print(len(training_set),len(validation_set),len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4: \n",
    "While Python’s SciKit-Learn library has a Na¨ıve Bayes classiﬁer, it works with continuous probability distributions and assumes numerical features. Although it is possible to transform categorical variables into numerical features using a binary encoding, we will instead build a simple Na¨ıve Bayes classiﬁer from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam:\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages)) # calculation of the \n",
    "        # probability of ham messages.\n",
    "        self.priors[1] = 1.0 - self.priors[0] # calculation of the probability of spam messages\n",
    "        self.likelihoods = [] # to build a frequency matrix\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages) # Using laplace estimator (1.0). \n",
    "                                                                                         #This calculates the conditional \n",
    "                                                                                         #probability P(words|ham) \n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages) # Using laplace estimator (1.0)\n",
    "                                                                                           #This calculates the conditional\n",
    "                                                                                           #probability P(words|spam)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)]) # adjusting the probability to reaching a \n",
    "                                                                            #maximum of 0.95 instead of 1\n",
    "        self.likelihoods = np.array (self.likelihoods).T  # result of the frequency matrix\n",
    "        \n",
    "\n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2: # checks if the probability of a word being a spam is 20 times higher than the probability\n",
    "                                   # of a word being a ham message.\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "    \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors) # to calculate the posterior probabilities of all new data points \n",
    "        for i, w in enumerate (self.words): # for loop \n",
    "            if w in message.lower():  # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:,i]  #if new words already exists in the frequency matrix  \n",
    "                                                    #to retrieve the posterior probability \n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise the new posterior probability\n",
    "        if posteriors[0] > 0.5: # classification of ham or spam\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2) # building the confusion matrix\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5:\n",
    "Explain the code: What is the purpose of each function? What do ’train’ and ‘train2’ do, and what is the diﬀerence between them? Where in the code is Bayes’ Theorem being applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. def train\n",
    "This code is the initial step of the Naive Bayes Classifier Algorithm which consists of constructing the frequency matrix.The code starts by searches for the prior probabilities of each word being either a spam or ham then it calculates the conditional probabilities of each word being either a spam or a ham in order to build the frequency matrix. In addition, the code uses the Laplace Estimator by adding one to every entry in the frequency matrix in order to avoid cases of null probabilities. It also adjusts cases of a probability being equal to 1 by adjusting it to 0.95.\n",
    "\n",
    "b. def train2\n",
    "This code is similar to first one (train) as it starts the same way but it modifies the frequency table by adding one if statement which consists of assigning a word as a spam if its probability is 20 times higher than being a ham message. \n",
    "\n",
    "c. def predict (### a large input needed here)\n",
    "After having trained the data with the two previous code, the Bayes theorem is applied in this algorithm. The third code consists of looking at the new data points to predict if these words are either ham or spam. If the new word is already\n",
    "present in the old data set then the code calculates the posterior probability through the frequency matrix if not the code assigns a new posterior probability and then normalize it. After assigning the posterior probabilities for the new words, they are classified as spam or ham.\n",
    "\n",
    "d. def score \n",
    "This algorithm evaluates the performance of the classification problem through a confusion matrix. In other words, the algorithm compares the actual class against the predicted class to calculate the performance measure of our prediction conducted in code number 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6:\n",
    "Use your training set to train the classiﬁers ‘train’ and ‘train2’. Note that the interfaces of our classiﬁers require you to pass the ham and spam messages separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ham = training_set.loc[training_set['label'] == 'ham']\n",
    "train_spam = training_set[training_set['label'] == 'spam']\n",
    "\n",
    "classifier_train1 = NaiveBayesForSpam()\n",
    "classifier_train1.train(train_ham['messages'].tolist(), train_spam['messages'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ham = training_set.loc[training_set['label'] == 'ham']\n",
    "train_spam = training_set[training_set['label'] == 'spam']\n",
    "\n",
    "\n",
    "classifier_train2 = NaiveBayesForSpam()\n",
    "classifier_train2.train2(train_ham['messages'].tolist(), train_spam['messages'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7:\n",
    "Using the validation set, explore how each of the two classiﬁers performs out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96203796203796199, array([[ 849.,   18.],\n",
       "        [  20.,  114.]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_train1.score(validation_set['messages'],validation_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96303696303696307, array([[ 863.,   31.],\n",
       "        [   6.,  101.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_train2.score(validation_set['messages'],validation_set['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train classifier has an accuracy of 0.96203796203796199 and train2 has an accuracy of 0.96303696303696307. Thus, train2 classifer performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 8:\n",
    "Why is the ‘train2’ classiﬁer faster? Why does it yield a better accuracy both on the training and the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train2 classifier is faster and yields a better accuracy for both the training and validation set than the train classifier due to its additional if function discussed previously in question 6. The train2 algorithm differs from train by the fact that it goes through a shorter list of spam key words (assigning a word as a spam if its probability is 20 times higher than being a ham message) rather than going through all of the words and their respective probabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 9:\n",
    "How many false positives (ham messages classiﬁed as spam messages) did you get in your validation set? How would you change the code to reduce false positives at the expense of possibly having more false negatives (spam messages classiﬁed as ham messages)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train classifier, we have got 20 false positives while for the train2 classifier we have got 6 false positives.\n",
    "The threshold of 0.5 in the code can be decreased to reduce false positives at the expense of possibly having more false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 10:\n",
    "Run the ‘train2’ classiﬁer on the test set and report its performance using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = classifier_train2.score(test_set['messages'],test_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "Confusion Matrix\n",
      "| /    | spam | ham |\n",
      "| spam | 865  | 32  |\n",
      "| ham  | 4    | 100 |\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "def print_table(table):\n",
    "    col_width = [max(len(str(x)) for x in col) for col in zip(*table)]\n",
    "    for line in table:\n",
    "        print(\"| \" + \" | \".join(\"{:{}}\".format(x, col_width[i])\n",
    "                                for i, x in enumerate(line)) + \" |\")\n",
    "\n",
    "print('Confusion Matrix')\n",
    "table = [['/', 'spam', 'ham'],\n",
    "         ['spam', '865', '32'],\n",
    "         ['ham', '4', '100']]\n",
    "\n",
    "print_table(table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_error_rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03596403596403597"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('total_error_rate:')\n",
    "(32+4)/ (865+32+4+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.964035964035964"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Accuracy:')\n",
    "1 - ((32+4)/ (865+32+4+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9643255295429208"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Sensitivity:')\n",
    "865/(865+32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9615384615384616"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Specificity:')\n",
    "100/(100+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---
title: "Machine_Learning_gw1_Daisy"
author: "xiaoyan zhou"
date: "2017Äê11ÔÂ29ÈÕ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
```

(a) Build a k-Nearest Neighbours classifier in R for ¡°wine_quality-white.csv¡± that:
1. loads the data file;
```{r}
winequality_white <- read_csv2("~/IC/Machine learning/groupwork 1/winequality-white.csv")
View(winequality_white)
```
2. construct a new binary column ¡°good wine¡± that indicates whether the wine is good
(which we define as having a quality of 6 or higher) or not;

```{r}
winequality_white["good_wine"] = 0

for (i in 1:nrow(winequality_white)){
  if (winequality_white[i, "quality"] >= 6){
    winequality_white[i, "good_wine"] = 1
  }else{
    winequality_white[i,"good_wine"] = 0
  }
}

```


4. normalises the data according to the Z-score transform;
```{r}
#convert chr to num
winequality_white[] <- lapply(winequality_white, function(x) as.numeric(as.character(x)))
#convert class to factor
winequality_white[["good_wine"]] = factor(winequality_white[["good_wine"]],labels = c("0","1"))
# str(winequality_white)

num_ww <- winequality_white
#normalise dataset
winequality_white["fixed acidity"] = scale(winequality_white["fixed acidity"])
winequality_white["volatile acidity"] = scale(winequality_white["volatile acidity"])
winequality_white["citric acid"] = scale(winequality_white["citric acid"])
winequality_white["residual sugar"] = scale(winequality_white["residual sugar"])
winequality_white["chlorides"] = scale(winequality_white["chlorides"])
winequality_white["free sulfur dioxide"] = scale(winequality_white["free sulfur dioxide"])
winequality_white["total sulfur dioxide"] = scale(winequality_white["total sulfur dioxide"])
winequality_white["density"] = scale(winequality_white["density"])
winequality_white$pH = scale(winequality_white$pH)
winequality_white["sulphates"] = scale(winequality_white["sulphates"])
winequality_white["alcohol"] = scale(winequality_white["alcohol"])

#delete the quality column

ww_data <- subset(winequality_white, select = -quality)
ww_predictor <- subset(ww_data, select = -good_wine)
```

3. splits the data set into a training data set (~40%), a validation data set (~30%) and a
test data set (~30%) ¡ª make sure you shuffle the record before the split;

```{r}
#install.packages("caTools")
#library(caTools)
set.seed(22)
split <- sample(1:3,size=nrow(ww_data),replace= TRUE,prob=c(0.4,0.3,0.3))
training <- ww_data[split == 1,]
validation <- ww_data[split == 2,]
testing <- ww_data[split == 3,]

# labels for 3 data set
train_labels <- training[["good_wine"]] #use [[]]to change it into a vector
valid_labels <- validation[["good_wine"]]
test_labels <- testing[["good_wine"]]

str(train_labels)
#eliminate the good_wine column in 3 set
traind <- subset(training, select = -good_wine)
valid <-subset(validation, select = -good_wine)
testd <-subset(testing, select = -good_wine)
```
5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;

```{r}
#Check if there are missing value
anyNA(ww_data)

#install.packages("class")
#install.packages("gmodels")
library(gmodels)
library(class)

#build a dataframe accuracy_k for saving k and the accuracy rate
accuracy_k <- data.frame(k = 1:80, misclassification_rate_t = rep(0,80),misclassification_rate_v = rep(0,80))  

for( i in 1:nrow(accuracy_k)){
   knn_i1 <- knn(train =traind, test = traind, cl = train_labels, k = i)
   mean_a <-mean(train_labels == knn_i1)
   accuracy_k[i, "misclassification_rate_t"] <- 1- mean_a
 
}


```


6. evaluates each classifier on the validation set and selects the best classifier;
```{r}

for( i in 1:nrow(accuracy_k)){
   knn_i1 <- knn(train =traind, test = valid, cl = train_labels, k = i)
   mean_a <-mean(valid_labels == knn_i1)
   accuracy_k[i, "misclassification_rate_v"] <- 1- mean_a
 
}


#find the optimal k with the largest accuracy rate
accuracy_k_optimal <- accuracy_k[order(-accuracy_k[ , "misclassification_rate_v"]), ]
accuracy_k_optimal[1, ] # the optimal model is the model with k =24, and the mean accuracy after repeating 3 times is 0.734.

accuracy_k_order <- accuracy_k[order(-accuracy_k[ , "k"]), ]


library(ggplot2)
ggplot(accuracy_k_order) + geom_line(aes(x = k, y = misclassification_rate_v)) +geom_line(aes(x = k, y = misclassification_rate_t)) +ggtitle('misclassification_rate for Different K')
knn_24 <- knn(train =traind, test = valid, cl = train_labels, k = 24)
CrossTable( x = valid_labels, y = knn_24, prop.chisp = FALSE)
mean(valid_labels == knn_24)
```


7. predicts the generalisation error using the test data set, as well as outputs the result
in a confusion matrix.

So a total of 1113 out of 1495, or 25.6 percent of wine were incorrectly classified by the k-NN
approach, where the k we chose is 24.

The cell percentages in the table indicate the proportion of values that fall into four
categories. The top-left cell indicates the true negative results. These 228 of 1495 values
are cases where the test_labels was "0" and the k-NN algorithm correctly identified it
as such. The bottom-right cell indicates the true positive results, where the classifier
and the test_labels agree that the good_wine is "1". A total of 885 of
1495 predictions were true positives.


```{r}
predict24 <- knn(train = traind, test = testd, cl = train_labels, k =24)
CrossTable( x = test_labels, y = predict24, prop.chisp = FALSE)
mean(test_labels == predict24)
```


How do you judge whether the classifier is well-suited for the data set?
```{r}
predict15 <- knn(train = traind, test = testd, cl = train_labels, k =15)
CrossTable( x = test_labels, y = predict15, prop.chisp = FALSE)
mean(test_labels == predict15)

predict20 <- knn(train = traind, test = testd, cl = train_labels, k =20)
CrossTable( x = test_labels, y = predict20, prop.chisp = FALSE)
mean(test_labels == predict20)

predict30 <- knn(train = traind, test = testd, cl = train_labels, k =30)
CrossTable( x = test_labels, y = predict30, prop.chisp = FALSE)
mean(test_labels == predict30)

table(train_labels) # good quality is important class

tbl15 <- table(test_labels,predict15)
sensitivity_15<-tbl24[2,2]/(tbl15[2,1]+tbl15[2,2])
specificity_15<-tbl15[1,1]/(tbl[1,1]+tbl15[1,2])


tbl20 <- table(test_labels,predict20)
sensitivity_20<-tbl20[2,2]/(tbl20[2,1]+tbl20[2,2])
specificity_20<-tbl20[1,1]/(tbl20[1,1]+tbl20[1,2])

tbl30 <- table(test_labels,predict30)
sensitivity_30<-tbl30[2,2]/(tbl30[2,1]+tbl30[2,2])
specificity_30<-tbl30[1,1]/(tbl30[1,1]+tbl30[1,2])
```
```{r}
ww_test_pred <- knn(train = ww_training_z, test = ww_test_z, cl = ww_train_label, k = 18)
CrossTable (x=ww_test_label, y=ww_test_pred, prop.chisq = FALSE)

gen_tbl <- table(ww_test_pred,ww_test_label)
ggplot(data=e, aes(x=k_count_1, misclass_err_1))+geom_line()+geom_line(data=d, aes(x=k_count, y=missclass_err))
accuracy <- (tbl[1, 1] + tbl[2, 2])/NROW(ww_training_z)

predict10 <- knn(train = ww_training_z, test =  ww_test_z, cl = train_labels, k =10)
CrossTable( x = ww_test_label, y = predict20, prop.chisp = FALSE)
mean(test_labels == predict10)

predict30 <- knn(train =  ww_training_z, test =  ww_test_z, cl = train_labels, k =30)
CrossTable( x = ww_test_label, y = predict30, prop.chisp = FALSE)
mean(test_labels == predict30)

table(train_labels) # good quality is important class

tbl10 <- table(ww_test_label,predict10)
specificity_10<-tbl24[2,2]/(tbl15[2,1]+tbl15[2,2])
sensitivity_10<-tbl15[1,1]/(tbl[1,1]+tbl15[1,2])


tbl18 <- table(ww_test_label,ww_test_pred)
specificity_20<-tbl18[2,2]/(tbl18[2,1]+tbl18[2,2])
sensitivity_20<-tbl18[1,1]/(tbl18[1,1]+tbl18[1,2])

tbl30 <- table(ww_test_label,predict30)
specificity_30<-tbl30[2,2]/(tbl30[2,1]+tbl30[2,2])
sensitivity_30<-tbl30[1,1]/(tbl30[1,1]+tbl30[1,2])
```
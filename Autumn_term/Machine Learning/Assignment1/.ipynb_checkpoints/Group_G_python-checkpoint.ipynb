{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment 1 Question 2 Python Version\n",
    "Group G  Joanna Andari, Karim Awad, Jiye Ren, Nirbhay Sharma, Qiuyue Zhang, Xiaoyan Zhou\n",
    "03/12/2017"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(b) Build a k-Nearest Neighbours classifier in Python for “wine_quality-red.csv”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\64254\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. loads the data file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_quality = pd.read_csv('winequality-red.csv',\n",
    "                          sep = ';')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. construct a new binary column “good wine” that indicates whether the wine is good (which we define as having a quality of 6 or higher) or not;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decide_good_wine(x):\n",
    "    if x >= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# good_wine = 1: it is good wine (quality >= 6)\n",
    "wine_quality['good_wine'] = wine_quality.quality.apply(decide_good_wine)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. splits the data set into a training data set (~50%) and a test data set (~50%) — make sure you shuffle the record before the split;\n",
    "and,\n",
    "4. normalises the whole data set (as Wolfram told to on the Hub) according to the Z-score transform;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_quality_mean = np.mean(wine_quality.iloc[:, 0:11], axis = 0)\n",
    "wine_quality_std = np.std(wine_quality.iloc[:, 0:11], axis = 0)\n",
    "wine_quality_normal = pd.DataFrame()\n",
    "for i in list(range(11)):\n",
    "    ap = ((wine_quality.iloc[:, i] - np.array(np.zeros(len(wine_quality.axes[0])) + wine_quality_mean[i])) / np.array(np.zeros(len(wine_quality.axes[0])) + wine_quality_std[i]))\n",
    "    wine_quality_normal = pd.concat([wine_quality_normal, ap], axis = 1)\n",
    "wine_quality_normal = pd.concat([wine_quality_normal, wine_quality.iloc[:, 11:13]], axis = 1)    \n",
    "\n",
    "random_seed = 100\n",
    "wine_quality_normal_shuffled = shuffle(wine_quality_normal, random_state = random_seed)\n",
    "training_set_normal = wine_quality_normal_shuffled[0:round(len(wine_quality_normal_shuffled.axes[0])/2)]\n",
    "test_set_normal = wine_quality_normal_shuffled[round(len(wine_quality_normal_shuffled.axes[0])/2):len(wine_quality_normal_shuffled.axes[0])]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. loads and trains the k-Nearest Neighbours classifiers for k = 1, 6, 11, 16, ..., 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNN_wine_classifier = []\n",
    "for i in list(range(1, 500, 5)):\n",
    "    KNN_wine_classifier.append(KNeighborsClassifier(n_neighbors = i))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. evaluates each classifier using 5-fold cross validation and selects the best classifier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performed classifier (with random seed set as 100) is the 48th classifier, with an average validation score of 0.749996191257.\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "for i in list(range(len(KNN_wine_classifier))):\n",
    "    scores.append(list(cross_val_score(KNN_wine_classifier[i], training_set_normal.iloc[:, 0:11], training_set_normal.iloc[:, 12], cv = 5)))\n",
    "# select the classifier with the higherest average score on the five folds\n",
    "avg_scores = [sum(scores[i])/5 for i in list(range(len(scores)))]\n",
    "max_score = max(avg_scores)\n",
    "max_score_pos = np.argmax(avg_scores)\n",
    "print('The best performed classifier (with random seed set as ' + str(random_seed) + ') is the ' + str(max_score_pos + 1) + 'th' + ' classifier, with an average validation score of ' + str(max_score) + '.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not good wine       0.67      0.75      0.71       365\n",
      "    Good wine       0.77      0.69      0.73       434\n",
      "\n",
      "  avg / total       0.72      0.72      0.72       799\n",
      "\n",
      "Confusion Matrix\n",
      "[[275  90]\n",
      " [136 298]]\n",
      "Total Error Rate = 0.282853566959\n",
      "Accuracy = 0.717146433041\n",
      "Sensitivity = 0.753424657534\n",
      "Specificity = 0.6866359447\n"
     ]
    }
   ],
   "source": [
    "# train this classifier again using all data (here the training_set_normal)\n",
    "KNN_wine_classifier[max_score_pos].fit(training_set_normal.iloc[:, 0:11], training_set_normal.iloc[:, 12])\n",
    "\n",
    "# prediction\n",
    "test_score = KNN_wine_classifier[max_score_pos].predict(test_set_normal.iloc[:, 0:11])\n",
    "print('Classification Report')\n",
    "\n",
    "print(classification_report(y_true = test_set_normal.iloc[:, 12],\n",
    "                            y_pred = test_score,\n",
    "                            target_names = ['Not good wine', 'Good wine']))\n",
    "\n",
    "x = confusion_matrix(y_true = test_set_normal.iloc[:, 12],\n",
    "                     y_pred = test_score)\n",
    "print('Confusion Matrix')\n",
    "print(x)\n",
    "print('Total Error Rate = ' + str((x[0, 1] + x[1, 0]) / sum(sum(x))))\n",
    "print('Accuracy = ' + str(1 - (x[0, 1] + x[1, 0]) / sum(sum(x))))\n",
    "print('Sensitivity = ' + str(x[0, 0] / (x[0, 0] + x[0, 1])))\n",
    "print('Specificity = ' + str(x[1, 1] / (x[1, 0] + x[1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do you judge whether the classifier is well-suited for the data set?\n",
    "\n",
    "A: After normalising the whole data set as specified by Wolfram on the Hub, we split the data into 5 subsets and train the models on 4 of the subsets while validate on the rest one. The models are all KNN method with the k set to be 1, 6, 11, ..., until 500 (100 in total). Then for each model, we calculate the average cross validation scores and choose the one with highest score. At last, after the best model is trained on all training data (50% of original data), we test it on the test data (the second half of randomly shuffled data) and obtain an estimated precision rate of approximately 0.72.\n",
    "   The accuracy of the selected classifier is the highest amoung all available classifiers. This classifier minimises the total error rate through choosing the k that minimises the sum of estimation bias and variance.\n",
    "   We want the specificity and sensitivity of the classifier to be close to 1, and also the balance of the two ratios is kept. Realistically, we cannot totally eliminate one in order to make another one equals 1 (e.g. classifying all wines as good wine). The ratios of the selected classifier are 0.7534 and 0.6866 respectively which is satisfyingly balanced as well as close to 1.\n",
    "   In terms of algorithm speed, KNN method has fast training phase as it basically just \"plots\" the training data points in the \"hypercube\" and has slow classification phase as it needs to calculate the distance between every pair of training-test nodes and makes decisions based on the number of categories & ranking of distances, which can incur huge amount of calculation when the data set is large. The purpose of our classifier is to decide whether a wine is \"good wine\" or not, and it would not be needed for a large amount (e.g. trillions) of data or data that are generated with a high frequncy (e.g. every millisecond). Therefore, the classification speed is not really a concern for this data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

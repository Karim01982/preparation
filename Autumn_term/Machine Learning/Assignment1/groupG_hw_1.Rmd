---
title: "Machine_Learning_gw1"
author: "xiaoyan zhou"
date: "2017Äê11ÔÂ29ÈÕ"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

(a) Build a k-Nearest Neighbours classifier in R for ¡°wine_quality-white.csv¡± that:
1. loads the data file;
```{r}
library(readr)
winequality_white <- read_csv2("~/IC/Machine learning/groupwork 1/winequality-white.csv")
View(winequality_white)
```
2. construct a new binary column ¡°good wine¡± that indicates whether the wine is good
(which we define as having a quality of 6 or higher) or not;

```{r}
winequality_white["good_wine"] = 0

for (i in 1:nrow(winequality_white)){
  if (winequality_white[i, "quality"] >= 6){
    winequality_white[i, "good_wine"] = 1
  }else{
    winequality_white[i,"good_wine"] = 0
  }
}

```


4. normalises the data according to the Z-score transform;
```{r}
#convert chr to num
winequality_white[] <- lapply(winequality_white, function(x) as.numeric(as.character(x)))


num_ww <- winequality_white
#normalise dataset
winequality_white["z_fixed_acidity"] = scale(winequality_white["fixed acidity"])
winequality_white["z_volatile_acidity"] = scale(winequality_white["volatile acidity"])
winequality_white["z_citric_acid"] = scale(winequality_white["citric acid"])
winequality_white["z_residual_sugar"] = scale(winequality_white["residual sugar"])
winequality_white["z_chlorides"] = scale(winequality_white["chlorides"])
winequality_white["z_free_sulfur_dioxide"] = scale(winequality_white["free sulfur dioxide"])
winequality_white["z_total_sulfur_dioxide"] = scale(winequality_white["total sulfur dioxide"])
winequality_white["z_density"] = scale(winequality_white["density"])
winequality_white$ph_z = scale(winequality_white$pH)
winequality_white["z_sulphates"] = scale(winequality_white["sulphates"])
winequality_white["z_alcohol"] = scale(winequality_white["alcohol"])

```

3. splits the data set into a training data set (~40%), a validation data set (~30%) and a
test data set (~30%) ¡ª make sure you shuffle the record before the split;

```{r}
#install.packages("caTools")
library(caTools)
set.seed(222)
split <- sample(1:3,size=nrow(winequality_white),replace=TRUE,prob=c(0.4,0.3,0.3))
training <- winequality_white[split ==1,]
validation <- winequality_white[split ==2,]
testing <- winequality_white[split ==3,]

```
5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;
```{r}
#Check if there are missing value
#install.packages("caret")
#library(caret)
anyNA(winequality_white)
summary(winequality_white)

training[["good_wine"]] = factor(training[["good_wine"]])

trctrl <- trainControl(method = "repeatedcv", number = 80, repeats = 3)
set.seed(3333)
knn_fit <- train(good_wine ~ z_fixed_acidity +z_volatile_acidity
                 + z_citric_acid + z_residual_sugar + z_chlorides
                 + z_chlorides +z_free_sulfur_dioxide + z_total_sulfur_dioxide
                 + z_density + ph_z +z_sulphates +z_alcohol, data = training, method = "knn",
 trControl=trctrl,
 tuneLength = 1)
knn_fit

```

6. evaluates each classifier on the validation set and selects the best classifier;
```{r}

```


7. predicts the generalisation error using the test data set, as well as outputs the result
in a confusion matrix.

```{r}

```


How do you judge whether the classifier is well-suited for the data set?



## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Group Assignment 2\n",
    "\n",
    "### Group G: Joanna Andari, Karim Awad, Jiye Ren, Nirbhay Sharma, Qiuyue Zhang, Xiaoyan Zhou\n",
    "#### 09/12/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from astropy.table import Table, Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: \n",
    "Load the data into a Python data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading\n",
    "messages = pd.read_csv('C:/Users/karim/Documents/Imperial/Machine Learning/ProblemSets/Assignment2/SMSSpamCollection.txt',sep='\\t', header=None,\n",
    "                           names=[\"label\", \"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: \n",
    "Pre-process the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lower case. (Please provide the Python code that achieves this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                           messages\n",
      "0   ham  go until jurong point  crazy   available only ...\n",
      "1   ham                      ok lar    joking wif u oni   \n",
      "2  spam  free entry in   a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor    u c already then say   \n",
      "4   ham  nah i don t think he goes to usf  he lives aro...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count                       5572\n",
       "unique                      5146\n",
       "top       sorry  i ll call later\n",
       "freq                          30\n",
       "Name: messages, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data processing\n",
    "def data_processing(p):\n",
    "    remove_number_punc = re.sub(\"[^a-zA-Z]\", \" \", p)\n",
    "    convert_to_lower_letter = remove_number_punc.lower()\n",
    "    return convert_to_lower_letter\n",
    "\n",
    "messages['messages'] = messages['messages'].apply(data_processing)\n",
    "print(messages.head())\n",
    "messages.groupby('label').describe()\n",
    "messages['messages'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: \n",
    "Shuﬄe the messages and split them into a training set (2,500 messages), a validation set (1,000 messages) and a test set (all remaining messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 1000 2072\n"
     ]
    }
   ],
   "source": [
    "#Data shuffling and segmenting\n",
    "random_seed = 100\n",
    "\n",
    "clean_messages_shuffled = shuffle(messages, random_state = random_seed)\n",
    "training_set = clean_messages_shuffled [0:round(len(clean_messages_shuffled.axes[0])/2.2288)]\n",
    "validation_set =clean_messages_shuffled [round(len(clean_messages_shuffled.axes[0])/2.2288):round(len(clean_messages_shuffled.axes[0])/1.592)]\n",
    "test_set=clean_messages_shuffled[round(len(clean_messages_shuffled.axes[0])/1.592):round(len(clean_messages_shuffled.axes[0])/1)]\n",
    "\n",
    "print(len(training_set),len(validation_set),len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4: \n",
    "While Python’s SciKit-Learn library has a Na¨ıve Bayes classiﬁer, it works with continuous probability distributions and assumes numerical features. Although it is possible to transform categorical variables into numerical features using a binary encoding, we will instead build a simple Na¨ıve Bayes classiﬁer from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam:\n",
    "    def train (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages)) # calculation of the \n",
    "        # probability of ham messages.\n",
    "        self.priors[1] = 1.0 - self.priors[0] # calculation of the probability of spam messages\n",
    "        self.likelihoods = [] # to build a frequency matrix\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages) # Using laplace estimator (1.0). \n",
    "                                                                                         #This calculates the conditional \n",
    "                                                                                         #probability P(words|ham) \n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages) # Using laplace estimator (1.0)\n",
    "                                                                                           #This calculates the conditional\n",
    "                                                                                           #probability P(words|spam)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)]) # adjusting the probability to reaching a \n",
    "                                                                            #maximum of 0.95 instead of 1\n",
    "        self.likelihoods = np.array (self.likelihoods).T  # result of the frequency matrix\n",
    "        \n",
    "\n",
    "    def train2 (self, hamMessages, spamMessages):\n",
    "        self.words = set (' '.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len (spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len (hamMessages)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) / len (spamMessages)\n",
    "            if prob1 * 20 < prob2: # checks if the probability of a word being a spam is 20 times higher than the probability\n",
    "                                   # of a word being a ham message.\n",
    "                self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "                spamkeywords.append (w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "\n",
    "    \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors) # to calculate the posterior probabilities of all new data points \n",
    "        for i, w in enumerate (self.words): # for loop \n",
    "            if w in message.lower():  # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:,i]  #if new words already exists in the frequency matrix  \n",
    "                                                    #to retrieve the posterior probability \n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i] #\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise the new posterior probability\n",
    "        if posteriors[0] > 0.5: # classification of ham or spam\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2) # building the confusion matrix\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5:\n",
    "Explain the code: What is the purpose of each function? What do ’train’ and ‘train2’ do, and what is the diﬀerence between them? Where in the code is Bayes’ Theorem being applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. def train\n",
    "This code is the initial step of the Naive Bayes Classifier Algorithm which consists of constructing the frequency matrix.The code starts by searches for the prior probabilities of each word being either a spam or ham then it calculates the conditional probabilities of each word being either a spam or a ham in order to build the frequency matrix. In addition, the code uses the Laplace Estimator by adding one to every entry in the frequency matrix in order to avoid cases of null probabilities. It also adjusts cases of a probability being equal to 1 by adjusting it to 0.95.\n",
    "\n",
    "b. def train2\n",
    "The Train 2 function is similar to the Train function, except it is not including ham messages with a conditional probability exceeding 0.05 (based on the if statement where prob1 * 20 < prob2). This is therefore assigning to the frequency matrix those spam words where the probability is close to 1, focusing our probabilities on words where there is a very high probability of the featured words being spam. This subsequently reduces the size of our frequency matrix, increasing both the accuracy of our findings and speed of calculation.\n",
    "\n",
    "\n",
    "c. def predict \n",
    "After having trained the data with the two previous code, the Bayes theorem is applied in this algorithm. The third code consists of looking at the new data points to predict if these words are either ham or spam. If the new word is already\n",
    "present in the old data set then the code calculates the posterior probability through the frequency matrix if not the code assigns a new posterior probability and then normalize it. After assigning the posterior probabilities for the new words, they are classified as spam or ham.\n",
    "\n",
    "d. def score \n",
    "This algorithm evaluates the performance of the classification problem through a confusion matrix. In other words, the algorithm compares the actual class against the predicted class to calculate the performance measure of our prediction conducted in code number 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6:\n",
    "Use your training set to train the classiﬁers ‘train’ and ‘train2’. Note that the interfaces of our classiﬁers require you to pass the ham and spam messages separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96138996138996136, array([[ 1758.,    44.],\n",
       "        [   36.,   234.]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ham = training_set.loc[training_set['label'] == 'ham']\n",
    "train_spam = training_set[training_set['label'] == 'spam']\n",
    "\n",
    "classifier_train1 = NaiveBayesForSpam()\n",
    "classifier_train1.train(train_ham['messages'].tolist(), train_spam['messages'].tolist())\n",
    "classifier_train1.score(test_set['messages'],test_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96718146718146714, array([[ 1785.,    59.],\n",
       "        [    9.,   219.]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ham = training_set.loc[training_set['label'] == 'ham']\n",
    "train_spam = training_set[training_set['label'] == 'spam']\n",
    "\n",
    "\n",
    "classifier_train2 = NaiveBayesForSpam()\n",
    "classifier_train2.train2(train_ham['messages'].tolist(), train_spam['messages'].tolist())\n",
    "classifier_train2.score(test_set['messages'],test_set['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7:\n",
    "Using the validation set, explore how each of the two classiﬁers performs out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96199999999999997, array([[ 848.,   18.],\n",
       "        [  20.,  114.]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_train1.score(validation_set['messages'],validation_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96299999999999997, array([[ 862.,   31.],\n",
       "        [   6.,  101.]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_train2.score(validation_set['messages'],validation_set['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train classifier has an accuracy of 0.96203796203796199 and train2 has an accuracy of 0.96303696303696307. Thus, train2 classifer performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 8:\n",
    "Why is the ‘train2’ classiﬁer faster? Why does it yield a better accuracy both on the training and the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train2 function is faster and yields a better accuracy for both the training and validation set than the train function due to the additional if statement discussed previously in question 6. The train2 function differs from the train function by going through a shorter list of key spam words with a high conditional probability (given our restriction that $Prob 1 *20$ <$Prob_2$ criteria), rather than going through all of the words (ie including those with a high conditional ham probability) and their respective probabilities. This increases our speed, as it reduces the size of our frequency matrix, given our focus on detecting whether a word is within a spam email or not, rather than also including words within ham emails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 9:\n",
    "How many false positives (ham messages classiﬁed as spam messages) did you get in your validation set? How would you change the code to reduce false positives at the expense of possibly having more false negatives (spam messages classiﬁed as ham messages)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to the output in Q7, for the train classifier, we have got 18 false positives while for the train2 classifier we have got 31 false positives.\n",
    "\n",
    "The posterior threshold of 0.5 within the predict function can be decreased to reduce false positives at the expense of possibly having more false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 10:\n",
    "Run the ‘train2’ classiﬁer on the test set and report its performance using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.96718146718146714, array([[ 1785.,    59.],\n",
      "       [    9.,   219.]]))\n"
     ]
    }
   ],
   "source": [
    "y = classifier_train2.score(test_set['messages'],test_set['label'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "Confusion Matrix\n",
      "| /           | Pred ham | Pred spam |\n",
      "| Actual ham  | 1785     | 59        |\n",
      "| Actual spam | 9        | 219       |\n",
      "total_error_rate:\n",
      "0.032818532818532815\n",
      "Accuracy:\n",
      "0.9671814671814671\n",
      "Sensitivity:\n",
      "0.9680043383947939\n",
      "Specificity:\n",
      "0.9605263157894737\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "def print_table(table):\n",
    "    col_width = [max(len(str(x)) for x in col) for col in zip(*table)]\n",
    "    for line in table:\n",
    "        print(\"| \" + \" | \".join(\"{:{}}\".format(x, col_width[i])\n",
    "                                for i, x in enumerate(line)) + \" |\")\n",
    "\n",
    "print('Confusion Matrix')\n",
    "table = [['/', 'Pred ham', 'Pred spam'],\n",
    "         ['Actual ham', '1785', '59'],\n",
    "         ['Actual spam', '9', '219']]\n",
    "\n",
    "print_table(table)\n",
    "\n",
    "print('total_error_rate:')\n",
    "print((59+9)/ (1785+59+9+219))\n",
    "\n",
    "print ('Accuracy:')\n",
    "print(1 - (59+9)/ (1785+59+9+219))\n",
    "\n",
    "print ('Sensitivity:')\n",
    "print(1785/(1785+59))\n",
    "\n",
    "print ('Specificity:')\n",
    "print(219/(219+9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The result on our test sample generates an accuracy level of 96.7%. Our sensitivity and specificity measures would suggest a marginally higher likelihood of classifying an actual spam message as ham (9 messages, generating a specificity of 0.961), although we note more instances (in absolute terms) of actual ham messages being predicted to be spam (59 text messages). As discussed above, we can adjust the posterior thresholds within our predict function to alter these measures, depending on what we determine to be more important (e.g. reducing spam in our inbox, or mis-categorising ham messages in our spam folders).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
